{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Novelty and Outlier Detection\n",
    "This notebook provides practical \"recipes\" for outplier and novelty detection problems. Each recipe includes explanations, code examples, visualizations, best practices, and common pitfalls when applicable. \n",
    "\n",
    "## Getting ready\n",
    "To begin, we’ll load a synthetic dataset and visualize what outliers might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate a synthetic dataset with intentional outliers\n",
    "X, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.60, random_state=2024)\n",
    "np.random.seed(2024)\n",
    "outliers = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
    "X_with_outliers = np.vstack([X, outliers])\n",
    "\n",
    "# Visualize the data with outliers\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], label=\"Normal Data\", alpha=0.6)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], color='red', label=\"Outliers\")\n",
    "plt.title(\"Synthetic Data with Outliers\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "To demonstrate outlier and novelty detection techniques, we’ll use `LocalOutlierFactor()` (or LOF) from `sklearn.neighbors`. It’s an unsupervised method that detects anomalies by comparing local densities of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Fit the model and predict outliers\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "y_pred = lof.fit_predict(X_with_outliers)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_with_outliers[y_pred == 1, 0], X_with_outliers[y_pred == 1, 1],\n",
    "            color='blue', label='Inliers', alpha=0.6)\n",
    "plt.scatter(X_with_outliers[y_pred == -1, 0], X_with_outliers[y_pred == -1, 1],\n",
    "            color='red', label='Outliers')\n",
    "plt.title(\"Outlier Detection using Local Outlier Factor\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Outlier and novelty detection methods estimate whether each sample data point significantly deviates from the expected data distribution.\n",
    "\n",
    "## Understanding Isolation Forest\n",
    "Isolation Forest is an efficient and scalable algorithm for detecting outliers in high-dimensional datasets. Rather than profiling normal data points and identifying deviations, it works by isolating anomalies. Outliers are easier to isolate because they tend to differ significantly from most of the data. The algorithm randomly selects a feature and splits the data based on a random threshold; fewer splits are typically needed to isolate anomalies.\n",
    "This method is particularly well-suited for large datasets and is capable of both outlier and novelty detection, making it a versatile tool in the machine learning toolkit.\n",
    "\n",
    "## Getting ready\n",
    "We’ll generate a synthetic dataset that includes visible outliers. This will allow us to compare the performance of Isolation Forest against the known distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data with outliers\n",
    "X, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.6, random_state=2024)\n",
    "np.random.seed(2024)\n",
    "outliers = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
    "X_combined = np.vstack([X, outliers])\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], label=\"Normal Data\", alpha=0.6)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], color='red', label=\"Outliers\")\n",
    "plt.title(\"Synthetic Data with Outliers\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We’ll use `IsolationForest()` from `sklearn.ensemble` to identify the anomalies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and configure the model\n",
    "model = IsolationForest(n_estimators=100, contamination=0.05, random_state=2024)\n",
    "\n",
    "# Fit the model and make predictions\n",
    "model.fit(X_combined)\n",
    "y_pred = model.predict(X_combined)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_combined[y_pred == 1, 0], X_combined[y_pred == 1, 1],\n",
    "            color='blue', label='Inliers', alpha=0.6)\n",
    "plt.scatter(X_combined[y_pred == -1, 0], X_combined[y_pred == -1, 1],\n",
    "            color='red', label='Outliers')\n",
    "plt.title(\"Outlier Detection using Isolation Forest\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Isolation Forest isolates anomalies instead of profiling normal observations. The algorithm works as follows:\n",
    "- Random Splitting: At each node, the algorithm randomly selects a feature and a split value. This continues recursively, building trees where anomalies are likely to be isolated more quickly.\n",
    "- Path Length: Each point’s anomaly score is based on the average path length over all trees. Shorter paths imply a higher likelihood of being an outlier.\n",
    "- Contamination: The contamination parameter determines the expected proportion of outliers in the dataset and sets a threshold for classification.\n",
    "Because Isolation Forest does not rely on distance or density metrics, it performs well on high-dimensional datasets and scales efficiently with large samples.\n",
    "\n",
    "## One-Class SVM for Novelty Detection\n",
    "One-Class Support Vector Machines (One-Class SVMs) are a prevailing technique for novelty detection, particularly when we only have access to \"normal\" data during training. Unlike other outlier detection methods, One-Class SVM attempts to learn the boundary of normality and classifies any point lying outside this boundary as a novelty. This makes it well-suited for use cases like fraud detection, equipment failure prediction, or rare disease diagnostics—any situation where anomalous examples are extremely rare or unavailable during training.\n",
    "One-Class SVM is a kernel-based method that can model nonlinear boundaries, giving it significant flexibility when separating normal instances from unseen anomalies.\n",
    "\n",
    "## Getting ready\n",
    "We’ll create a dataset with normal training data and include anomalies only at test time to demonstrate novelty detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate training data (normal only)\n",
    "X_train, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.6, random_state=2024)\n",
    "\n",
    "# Generate test data (normal + novelty)\n",
    "X_test_normal, _ = make_blobs(n_samples=100, centers=1, cluster_std=0.6, center_box=(0, 1), random_state=42)\n",
    "X_test_novelty = np.random.uniform(low=-6, high=6, size=(20, 2))\n",
    "X_test = np.vstack([X_test_normal, X_test_novelty])\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], label='Training Data', alpha=0.6)\n",
    "plt.scatter(X_test_novelty[:, 0], X_test_novelty[:, 1], color='red', label='Novelty Points')\n",
    "plt.title(\"Training Data and Novelty Points\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We’ll now use `OneClassSVM()` from `sklearn.svm` to build a novelty detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the model\n",
    "model = OneClassSVM(kernel='rbf', gamma='scale', nu=0.05)\n",
    "model.fit(X_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test[y_pred == 1, 0], X_test[y_pred == 1, 1], color='blue', label='Predicted Inliers', alpha=0.6)\n",
    "plt.scatter(X_test[y_pred == -1, 0], X_test[y_pred == -1, 1], color='red', label='Predicted Novelties')\n",
    "plt.title(\"Novelty Detection using One-Class SVM\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "One-Class SVM separates normal data from the origin in a transformed feature space using a decision function. Key parameters include:\n",
    "- Kernel: One-Class SVM can use linear, polynomial, or RBF kernels. RBF is most common as it can model non-linear boundaries.\n",
    "- Gamma: Defines the influence of a single training example. A low value means ‘far’ reach, high value means ‘close’ reach.\n",
    "-Nu: An upper bound on the fraction of training errors and a lower bound on the fraction of support vectors. It controls the trade-off between false positives and model flexibility.\n",
    "Unlike outlier detection methods that look at all data (training + test), One-Class SVM learns from normal data only and is evaluated only on new, unseen points.\n",
    "\n",
    "## Local Outlier Factor (LOF)\n",
    "Local Outlier Factor (LOF) is a density-based anomaly detection method that identifies outliers by comparing the local density of each data point to that of its neighbors. Rather than using a global threshold, LOF assesses how isolated a data point is with respect to the surrounding neighborhood. If a point lies in a region of significantly lower density than its neighbors, it is flagged as an outlier.\n",
    "LOF is especially effective in datasets where the density of data points varies across the feature space. It can detect local anomalies that may be overlooked by global methods like Isolation Forest.\n",
    "\n",
    "## Getting ready\n",
    "We’ll generate a dataset containing clusters with different densities and add noise to simulate outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Create synthetic data with clusters and noise\n",
    "X, _ = make_blobs(n_samples=400, centers=[[0, 0], [5, 5]], cluster_std=[0.5, 1.5], random_state=2024)\n",
    "np.random.seed(2024)\n",
    "outliers = np.random.uniform(low=-6, high=10, size=(20, 2))\n",
    "X_with_outliers = np.vstack([X, outliers])\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], label=\"Clustered Data\", alpha=0.6)\n",
    "plt.scatter(outliers[:, 0], outliers[:, 1], color='red', label=\"Outliers\")\n",
    "plt.title(\"Synthetic Data with LOF-Detectable Outliers\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We’ll apply `LocalOutlierFactor()` from `sklearn.neighbors` to identify local anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the model\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "y_pred = lof.fit_predict(X_with_outliers)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_with_outliers[y_pred == 1, 0], X_with_outliers[y_pred == 1, 1],\n",
    "            color='blue', label='Inliers', alpha=0.6)\n",
    "plt.scatter(X_with_outliers[y_pred == -1, 0], X_with_outliers[y_pred == -1, 1],\n",
    "            color='red', label='Outliers')\n",
    "plt.title(\"Local Outlier Factor Anomaly Detection\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "LOF evaluates the degree of abnormality of a data point by:\n",
    "- Local Density Estimation: It computes the local density around a data point using the distance to its k-nearest neighbors (refer to Chapter 4 for a refresher).\n",
    "- Reachability Distance: The reachability distance smooths distances by considering the maximum between the actual distance and the k-distance of the neighbor.\n",
    "- LOF Score: A point’s LOF score is the ratio of the average local density of its neighbors to its own local density. Values much greater than 1 suggest “outlierness.”\n",
    "Unlike other models, LOF does not expose a `.predict()` method for unseen data. It is strictly an unsupervised anomaly detector used at training time.\n",
    "\n",
    "## Evaluating Outlier Detection Models\n",
    "Evaluating outlier detection models is more nuanced than evaluating traditional supervised models. Outliers are typically rare, and labels may not always be available, which limits the use of standard metrics like accuracy. Instead, we use metrics suited for imbalanced datasets and binary decisions, such as precision, recall, F1-score, ROC-AUC, and confusion matrices – all of which we’ve utilized several times up to this point. When true labels are available (as in synthetic datasets), we can directly assess how our models identify anomalous points.\n",
    "In this section, we’ll walk through evaluation strategies for outlier detection models using labeled data, compare model performance, and visualize the results for interpretability.\n",
    "\n",
    "## Getting ready\n",
    "We’ll generate a labeled dataset with a clear distinction between inliers and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate labeled synthetic data\n",
    "X_inliers, _ = make_blobs(n_samples=300, centers=[[0, 0]], cluster_std=0.6, random_state=2024)\n",
    "X_outliers = np.random.uniform(low=-6, high=6, size=(30, 2))\n",
    "X = np.vstack([X_inliers, X_outliers])\n",
    "y_true = np.array([0] * len(X_inliers) + [1] * len(X_outliers))  # 0 = inlier, 1 = outlier\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y_true == 0][:, 0], X[y_true == 0][:, 1], label='Inliers', alpha=0.6)\n",
    "plt.scatter(X[y_true == 1][:, 0], X[y_true == 1][:, 1], color='red', label='True Outliers')\n",
    "plt.title(\"Synthetic Labeled Data for Evaluation\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We’ll fit an Isolation Forest model and evaluate its predictions using classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fit the model and get predictions\n",
    "model = IsolationForest(contamination=0.09, random_state=2024)\n",
    "model.fit(X)\n",
    "y_pred = model.predict(X)\n",
    "y_pred_binary = np.where(y_pred == 1, 0, 1)  # convert to 0 = inlier, 1 = outlier\n",
    "\n",
    "# Generate and plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted Inlier', 'Predicted Outlier'],\n",
    "            yticklabels=['True Inlier', 'True Outlier'])\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report as a styled DataFrame and AUC\n",
    "report = classification_report(y_true, y_pred_binary, output_dict=True)\n",
    "report = pd.DataFrame(report).transpose()\n",
    "styled_report = (report\n",
    "    .style\n",
    "    .background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score'])\n",
    "    .format({\n",
    "        'precision': '{:.3f}',\n",
    "        'recall': '{:.3f}',\n",
    "        'f1-score': '{:.3f}',\n",
    "        'support': '{:.0f}'\n",
    "    })\n",
    ")\n",
    "display(styled_report)\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_true, model.decision_function(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Each metric offers a unique view of model performance:\n",
    "- Precision: The proportion of predicted outliers that are actually true outliers. High precision indicates fewer false positives.\n",
    "- Recall: The proportion of actual outliers that are correctly identified. High recall means fewer false negatives.\n",
    "- F1-Score: The harmonic mean of precision and recall. A balanced measure when dealing with class imbalance.\n",
    "- Confusion Matrix: Offers detailed insight into prediction distribution.\n",
    "- ROC-AUC Score: Measures the model’s ability to rank true outliers above inliers.\n",
    "When class labels are not available (which is common), model evaluation may rely on expert validation, reconstruction error (e.g., for autoencoders), or proxy metrics like prediction stability.\n",
    "\n",
    "## Handling Detected Outliers\n",
    "Once outliers have been identified, we face an important decision: how should we handle them? The appropriate strategy depends on the context of the problem and the nature of the data. Outliers can be informative (e.g., fraud cases) or disruptive (e.g., sensor glitches) and choosing how to treat them affects model performance and interpretability.\n",
    "This section outlines common strategies for handling outliers, including removal, transformation, imputation, and retaining them for specialized modeling. We’ll walk through practical code examples to demonstrate each approach.\n",
    "\n",
    "## Getting ready\n",
    "We’ll use a dataset that includes outliers detected via the Isolation Forest method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate the dataset\n",
    "X_inliers, _ = make_blobs(n_samples=300, centers=[[0, 0]], cluster_std=0.6, random_state=2024)\n",
    "X_outliers = np.random.uniform(low=-6, high=6, size=(30, 2))\n",
    "X = np.vstack([X_inliers, X_outliers])\n",
    "\n",
    "#  Detect outliers using Isolation Forest\n",
    "model = IsolationForest(contamination=0.09, random_state=2024)\n",
    "model.fit(X)\n",
    "outlier_mask = model.predict(X) == -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We’ll now explore different strategies for handling the detected outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers from the dataset\n",
    "X_cleaned = X[~outlier_mask]\n",
    "\n",
    "# Replace outliers with the feature-wise median\n",
    "X_replaced = X.copy()\n",
    "median_vals = np.median(X[~outlier_mask], axis=0)\n",
    "X_replaced[outlier_mask] = median_vals\n",
    "\n",
    "# Cap outliers to specified bounds (winsorization)\n",
    "X_df = pd.DataFrame(X, columns=['feature1', 'feature2'])\n",
    "X_capped = X_df.copy()\n",
    "for col in X_df.columns:\n",
    "    lower = X_df[col].quantile(0.05)\n",
    "    upper = X_df[col].quantile(0.95)\n",
    "    X_capped[col] = X_df[col].clip(lower=lower, upper=upper)\n",
    "\n",
    "# Visualize original and cleaned datasets\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], label='Original Data', alpha=0.6)\n",
    "plt.scatter(X[outlier_mask][:, 0], X[outlier_mask][:, 1], color='red', label='Detected Outliers')\n",
    "plt.title('Original Dataset with Outliers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_cleaned[:, 0], X_cleaned[:, 1], label='Cleaned Data', alpha=0.6)\n",
    "plt.title('Dataset After Outlier Removal')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "The choice of strategy depends on the underlying assumptions and the impact of outliers on model accuracy:\n",
    "- Removal: Appropriate when outliers result from noise, errors, or corrupted records. However, excessive removal may discard useful signal.\n",
    "- Replacement: Replaces outliers with the mean, median, or another central statistic. This preserves dataset size but may introduce bias.\n",
    "- Capping (Winsorization): Restricts values to a specific range. Useful when values should fall within bounds but still retain ordering.\n",
    "- Flagging: Outliers can also be retained but flagged as a new feature, allowing models to learn behavior differences.\n",
    "\n",
    "## Choosing the Right Detection Technique\n",
    "With multiple approaches to outlier and novelty detection available in scikit-learn, selecting the right method depends on the nature of your dataset, the presence or absence of labels, and the use case. In this section, we’ll compare the key characteristics of various detection algorithms covered in this chapter and provide practical guidance to help you determine which approach best suits your needs.\n",
    "We'll explore decision criteria including assumptions about the data distribution, scalability, dimensionality, interpretability, and whether the model supports predictions on new, unseen data.\n",
    "\n",
    "## Getting ready\n",
    "We’ll summarize our experimental setup and reuse model evaluation results from earlier sections. (NOTE: This part is really OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Create a comparison table of methods\n",
    "data = {\n",
    "    \"Algorithm\": [\"Isolation Forest\", \"Local Outlier Factor\", \"One-Class SVM\"],\n",
    "    \"Handles High Dimensions\": [\"Yes\", \"No\", \"Sometimes\"],\n",
    "    \"Learns From Unlabeled Data\": [\"Yes\", \"Yes\", \"No (novelty only)\"],\n",
    "    \"Supports New Predictions\": [\"Yes\", \"No\", \"Yes\"],\n",
    "    \"Scales to Large Data\": [\"Yes\", \"No\", \"No\"],\n",
    "    \"Robust to Varying Densities\": [\"No\", \"Yes\", \"Partially\"],\n",
    "    \"Main Strength\": [\n",
    "        \"Fast, scalable, easy to use\",\n",
    "        \"Detects local anomalies in varying densities\",\n",
    "        \"Kernel-based boundary modeling\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(data)\n",
    "summary_df.set_index(\"Algorithm\", inplace=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We’ll walk through example-driven decision criteria to choose the right method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your data is large and high-dimensional, consider:\n",
    "# Isolation Forest is preferred\n",
    "model = IsolationForest(contamination=0.1, random_state=2024)\n",
    "\n",
    "#If your data has clusters with varying densities:\n",
    "# Local Outlier Factor is better suited\n",
    "model = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "\n",
    "# If you want to train on only normal data and detect novel data points later:\n",
    "# One-Class SVM enables novelty detection\n",
    "model = OneClassSVM(kernel='rbf', gamma='scale', nu=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Each algorithm has its strengths and limitations:\n",
    "- Isolation Forest: Ideal for general-purpose outlier detection with large, high-dimensional datasets. It isolates anomalies quickly but may underperform with varying density regions.\n",
    "- Local Outlier Factor (LOF): Detects local deviations, making it powerful in non-uniform data. However, it cannot make predictions on unseen data.\n",
    "- One-Class SVM: Suited for novelty detection where only normal data is available during training. It can be computationally expensive on large datasets and is sensitive to feature scaling.\n",
    "When in doubt, start with Isolation Forest for efficiency and iterate based on domain-specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises in Novelty and Outlier Detection\n",
    "In this final section, we’ll engage in practical exercises that involve detecting, evaluating, and handling anomalies in real-world datasets. These exercises are designed to reinforce the concepts introduced throughout the chapter—ranging from model selection to evaluation and strategy implementation. By the end of this section, you’ll have direct experience working with a variety of detection methods and be better equipped to select and fine-tune them based on your data and goals.\n",
    "\n",
    "### Exercise 1: Applying Isolation Forest to a Real-World Dataset\n",
    "In this exercise, we’ll detect outliers in a credit card transaction dataset using the Isolation Forest algorithm.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Generate synthetic data with inliers and outliers\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Scale the data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Apply Isolation Forest\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Evaluate the results\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Print classification report as a styled DataFrame\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Using LOF on a Network Intrusion Dataset\n",
    "In this task, we simulate an imbalanced network-like dataset with several clusters and injected noise. You'll apply the Local Outlier Factor algorithm to identify low-density regions where anomalous behavior might occur. Finally, you'll evaluate model performance using a confusion matrix and classification metrics.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Generate clustered data with synthetic noise\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Scale the data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Apply Local Outlier Factor\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Evaluate the predictions\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: One-Class SVM for Manufacturing Sensor Data\n",
    "This exercise demonstrates novelty detection in a simulated manufacturing environment. You’ll train a model only on normal operational data, then use it to detect novel observations from a mix of normal and abnormal test data. Finally, you’ll visualize the inliers and detected novelties in 2D feature space.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Generate normal and novel data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Fit One-Class SVM on normal data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Visualize predictions\n",
    "YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
