{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: Clustering Techniques\n",
    "This notebook provides practical \"recipes\" for text classification and multiclass classification problems. Each recipe includes explanations, code examples, visualizations, best practices, and common pitfalls when applicable. \n",
    "\n",
    "## Getting ready\n",
    "We’ll start by creating some dummy data with scikit-learn’s make_blobs() function (which is very appropriately named)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the dataset\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=2024)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do it...\n",
    "Let’s have a look at when we generated. You’ll also notice that we applied the StandardScaler() in order to transform our values using z-scores. This will make our clusters easier to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Sample Dataset for Clustering\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Clustering works by calculating the similarity between data points based on distance metrics or density measures. Depending on the algorithm, the approach to determining clusters can vary significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means Clustering\n",
    "K-means is a centroid-based clustering algorithm that partitions data into a predefined number of clusters (which is perfect considering our data is quite “blobby” from the previous section). First, k-means randomly creates centroids in our feature space. Next, it iteratively assigns each data point to the nearest cluster centroid and then recalculates the cluster centroids and moves them in the feature space so that they are positioned approximately within the average distance among the data points current assigned to them in the current iteration. This process continues until “convergence” where the centroids don’t move much and data points are not being reassigned to other cluster centroid. K-means is efficient and works best when clusters are convex, isotropic, and roughly equal in size…which also can be its greatest weakness.\n",
    "Getting ready\n",
    "Here, we’ll use the previous dummy data and just load in the KMeans() function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "from sklearn.cluster import KMeans\n",
    "# Reuse the dataset from the previous section\n",
    "# Dataset already loaded and scaled as X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "To apply k-means, we have to simply provide n_clusters as our only required argument (the “k” in “k-means” is a reference to this cluster number). This initializes the same number of centroids when we begin the iterative process described earlier. This also implies that we know or at least can estimate the number of clusters beforehand. This could also be a potential limitation of k-means if we don’t have an estimate to begin with. However, typically we take steps to refine this value as we’ll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=2024)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75, marker='X')\n",
    "plt.title(\"K-Means Clustering Results\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "K-Means follows an iterative refinement approach:\n",
    "- Initialization: The centroids of “k” clusters are initialized randomly or using methods like 'k-means++'.\n",
    "- Assignment Step: Each data point is assigned to the nearest centroid based on a distance metric (typically Euclidean distance which uses the common straight-line distance described using x/y coordinates).\n",
    "- Update Step: The centroid of each cluster is recalculated as the mean of all assigned points.\n",
    "This process continues until the assignments no longer change (convergence) or a maximum number of iterations is reached. K-means minimizes within-cluster variance (inertia).\n",
    "This plot displays the four clusters found by K-Means, along with their respective centroids marked by black X's.\n",
    "\n",
    "There’s more...\n",
    "How do you choose the optimal number of clusters? One popular method to determine the best value of “k” is the elbow method. It involves plotting the sum of squared distances (inertia) for a range of “k” values and looking for the point where adding more clusters yields diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot clustering inertia\n",
    "inertia = []\n",
    "k_values = range(1, 10)\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=2024)\n",
    "    km.fit(X)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "plt.plot(k_values, inertia, 'bo-')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "Hierarchical clustering builds nested clusters by either merging or splitting them successively. It is especially useful when the number of clusters is not known in advance, and it provides a tree-like structure (dendrogram) that visually conveys relationships among the data. This is an improvement on k-means that requires a value for “k” prior to execution. There are two main approaches: agglomerative (bottom-up) and divisive (top-down). In practice, agglomerative clustering is more commonly used and supported directly in scikit-learn.\n",
    "\n",
    "## Getting ready\n",
    "As before, we can use the same dataset we created earlier and simply apply the new technique to it by importing the scikit-learn class and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Reuse the dataset from the earlier section\n",
    "# Dataset already loaded and scaled as X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Here, we apply agglomerative clustering. We can specify a number of clusters, or we can specify a distance metric threshold to create a cutoff point for the number of clusters returned – one or the other, but in its purest form, no set number of clusters is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply agglomerative clustering\n",
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "y_agg = agg.fit_predict(X)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_agg, cmap='viridis', s=50)\n",
    "plt.title(\"Agglomerative Clustering Results\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "# Generate the dendrogram using SciPy\n",
    "linked = linkage(X, method='ward')\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linked, leaf_rotation=90, leaf_font_size=8)\n",
    "plt.title(\"Dendrogram for Hierarchical Clustering\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Agglomerative clustering follows a bottom-up approach:\n",
    "- Each data point starts as its own cluster.\n",
    "- At each step, the two closest clusters are merged based on a linkage criterion.\n",
    "- This continues until all points are merged into a single cluster or until a stopping condition (such as a predefined number of clusters) is met.\n",
    "The linkage method defines how the distance between clusters is calculated:\n",
    "- Ward: Minimizes the variance of the merged clusters (default and most commonly used)\n",
    "- Complete: Maximum distance between points in two clusters\n",
    "- Average: Average distance between all pairs of points\n",
    "- Single: Minimum distance between points\n",
    "The dendrogram visualizes the clustering process and can guide the selection of the number of clusters by choosing a height to \"cut\" the tree.\n",
    "\n",
    "## There's more...\n",
    "Dendrograms are powerful for understanding the structure of the data. If clusters are well-separated, the height of the linkage distances will show clear gaps. This can help in determining whether a natural grouping exists in the data.\n",
    "While scikit-learn does not provide native support for divisive clustering (top-down), similar approaches can be implemented using other tools or custom algorithms. Divisive clustering starts with all points in one cluster and recursively splits them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density-Based Clustering with DBSCAN\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a somewhat unique clustering algorithm capable of identifying clusters of varying shapes and sizes. It differs from K-means and hierarchical clustering by not requiring the number of clusters to be specified in advance and by handling outliers (noise) effectively. This means, unlike k-means, it does not generate centroids a priori.\n",
    "\n",
    "## Getting ready\n",
    "Here, we are going to use another data generator function in scikit-learn called make_moons(), which, again, like make_blobs() is aptly named!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Create a new one with noise\n",
    "X, _ = make_moons(n_samples=300, noise=0.1, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Let’s execute DBSCAN. In this example we are setting the two most important arguments, eps and min_samples, to 0.2 and 5, respectively. More on these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "y_db = dbscan.fit_predict(X)\n",
    "\n",
    "# Plot the DBSCAN results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_db, cmap='plasma', s=50)\n",
    "plt.title(\"DBSCAN Clustering Results\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "DBSCAN groups data points based on two main parameters:\n",
    "- eps: The radius of the neighborhood around a point.\n",
    "- min_samples: The minimum number of points required to form a dense region.\n",
    "Points are classified into three categories:\n",
    "- Core points: Have at least min_samples within eps distance.\n",
    "- Border points: Fall within the eps neighborhood of a core point but have fewer than min_samples neighbors themselves.\n",
    "- Noise points: Neither core nor border points.\n",
    "DBSCAN starts from an unvisited point, checks its “neighborhood,” and forms clusters by recursively including density-connected neighbors. It’s particularly good at identifying non-spherical clusters and separating noise which is highly valuable in real-world applications where data is often “dirty.”\n",
    "\n",
    "## There’s more...\n",
    "How do you choose eps and min_samples? The performance of DBSCAN is sensitive to the choice of eps and min_samples. A k-distance plot can help determine a good value for eps. Plot the distance to the k-th nearest neighbor (where k = min_samples) for all points and look for a sharp bend (elbow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Generate Nearest Neighbors\n",
    "neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors_fit = neighbors.fit(X)\n",
    "distances, indices = neighbors_fit.kneighbors(X)\n",
    "distances = np.sort(distances[:, 4])\n",
    "\n",
    "# Plot k-distance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(distances)\n",
    "plt.title(\"k-Distance Plot (k=5)\")\n",
    "plt.xlabel(\"Points sorted by distance\")\n",
    "plt.ylabel(\"5th Nearest Neighbor Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Evaluation Metrics\n",
    "Evaluating the results of clustering is crucial to assess the quality and relevance of the groupings discovered by unsupervised algorithms. However, unlike supervised learning, clustering lacks true labels or target values we’re trying to predict, so we rely on internal and external evaluation metrics such as the silhouette score, Davies-Bouldin index, and adjusted Rand index to determine how well the model has performed. Again, with unsupervised learning techniques, evaluation can be seen as more of an art than science, but we can still make educated decisions with the right tools.\n",
    "\n",
    "## Getting ready\n",
    "To begin, we’ll load our evaluation metrics, create a dummy data set and fit a k-means clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate a labeled dataset for evaluation\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=2024)\n",
    "\n",
    "# Fit KMeans for use with evaluation metrics\n",
    "kmeans = KMeans(n_clusters=4, random_state=2024)\n",
    "y_kmeans = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Applying the evaluation techniques is straightforward and simply takes the input data and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the silhouette score\n",
    "sil_score = silhouette_score(X, y_kmeans)\n",
    "print(f\"Silhouette Score: {sil_score:.3f}\")\n",
    "\n",
    "# Calculate the Davies-Bouldin index\n",
    "db_index = davies_bouldin_score(X, y_kmeans)\n",
    "print(f\"Davies-Bouldin Index: {db_index:.3f}\")\n",
    "\n",
    "# (Optional) Calculate adjusted Rand index if ground truth is known\n",
    "ari = adjusted_rand_score(y_true, y_kmeans)\n",
    "print(f\"Adjusted Rand Index: {ari:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "- **Silhouette Score**: measures how similar a point is to its own cluster compared to other clusters. It ranges from -1 (poor clustering) to 1 (dense and well-separated clusters). A higher value indicates better-defined clusters.\n",
    "- **Davies-Bouldin Index**: evaluates the average similarity ratio of each cluster with its most similar one. Lower values indicate better clustering. Unlike the silhouette score, this metric penalizes overlapping clusters.\n",
    "- **Adjusted Rand Index (ARI)**: compares the clustering result to a known ground truth by examining all pairs of samples and counting pairs assigned to the same or different clusters. It corrects for chance and ranges from -1 to 1, where 1 indicates perfect agreement.\n",
    "\n",
    "## There’s more...\n",
    "Additional metrics to consider include Calinski-Harabasz Index.  This is also called the variance ratio criterion and it favors well-separated and dense clusters. Like the Davies-Bouldin index, it is computed using intra-cluster and inter-cluster dispersion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the Right Clustering Algorithm\n",
    "Selecting the most suitable clustering algorithm depends heavily on the structure and properties of the dataset. There’s no one-size-fits-all solution—different algorithms are suited to different types of data distributions, levels of noise, and dimensionality! This section compares key characteristics of clustering algorithms and provides guidance for choosing among them.\n",
    "\n",
    "## Getting ready\n",
    "Let’s begin by creating a variety of dummy datasets using scikit-learn functions we’ve used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create and scale different datasets\n",
    "X_blobs, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=2024)\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.1, random_state=2024)\n",
    "X_circles, _ = make_circles(n_samples=300, noise=0.05, factor=0.5, random_state=2024)\n",
    "\n",
    "X_blobs = StandardScaler().fit_transform(X_blobs)\n",
    "X_moons = StandardScaler().fit_transform(X_moons)\n",
    "X_circles = StandardScaler().fit_transform(X_circles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to do it...\n",
    "Each data structure lends itself to a particular clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the datasets\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axs[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c='blue', alpha=0.6)\n",
    "axs[0].set_title(\"Isotropic Gaussian Blobs\")\n",
    "axs[1].scatter(X_moons[:, 0], X_moons[:, 1], c='green', alpha=0.6)\n",
    "axs[1].set_title(\"Moons\")\n",
    "axs[2].scatter(X_circles[:, 0], X_circles[:, 1], c='purple', alpha=0.6)\n",
    "axs[2].set_title(\"Nested Circles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "When selecting a clustering algorithm, consider the following factors:\n",
    "- Cluster Shape: K-Means performs well with convex clusters. DBSCAN or spectral clustering are better suited for non-convex shapes.\n",
    "- Noise Handling: DBSCAN explicitly handles noise by identifying outliers as separate from clusters. K-Means and hierarchical clustering assign all points to a cluster.\n",
    "- Scalability: K-Means scales well to large datasets. Hierarchical clustering is less scalable due to its time complexity.\n",
    "- Parameter Sensitivity:\n",
    "  - K-Means: sensitive to k and initialization.\n",
    "  - DBSCAN: sensitive to eps and min_samples.\n",
    "  - Agglomerative clustering: affected by linkage method and stopping criteria.\n",
    "- Dimensionality: Clustering in high dimensions often requires dimensionality reduction (e.g., PCA) beforehand due to the curse of dimensionality.\n",
    "\n",
    "## There’s more...\n",
    "Choosing your clustering technique should be based on the characteristics of your data. It’s always good to plot your raw dataset beforehand to try to identify any underlying structure that can guide your approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Clustering Techniques\n",
    "Beyond the “classic” clustering algorithms like, scikit-learn offers several advanced techniques such as Spectral Clustering and Gaussian Mixture Models (GMMs). These methods provide more flexibility in modeling complex cluster shapes and probabilistic cluster assignments, making them useful for more nuanced tasks.\n",
    "\n",
    "## Getting ready\n",
    "Let’s begin by loading our two new clustering models and generating some dummy data to test them on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# Create datasets with complex structure\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, _ = make_moons(n_samples=300, noise=0.05, random_state=2024)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Each model is fit in a similar fashion as the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Spectral Clustering\n",
    "spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=2024)\n",
    "y_spectral = spectral.fit_predict(X)\n",
    "\n",
    "# Apply Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=2, random_state=2024)\n",
    "y_gmm = gmm.fit_predict(X)\n",
    "\n",
    "# Visualize both clustering results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=y_spectral, cmap='plasma', s=50)\n",
    "ax1.set_title(\"Spectral Clustering\")\n",
    "\n",
    "ax2.scatter(X[:, 0], X[:, 1], c=y_gmm, cmap='viridis', s=50)\n",
    "ax2.set_title(\"Gaussian Mixture Model\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Let’s take a closer look at each of these techniques to understand how they work.\n",
    "\n",
    "### Spectral Clustering\n",
    "- Constructs a similarity graph from the data.\n",
    "- Constructs a matrix that represents the relationships between the data points.\n",
    "- Maps this high-dimensional graph to a lower-dimensional feature space.\n",
    "- Applies K-means on the reduced representation.\n",
    "This method is effective for non-convex clusters and is influenced by the similarity measure (e.g., radial basis, nearest neighbors). It also works well on large datasets with many features.\n",
    "\n",
    "### Gaussian Mixture Models (GMMs)\n",
    "- GMMs are a “soft clustering” method based on the assumption that data is generated from a mixture of several Gaussian (or normal) distributions.\n",
    "- Each point is assigned a probability of belonging to each cluster.\n",
    "- The Expectation-Maximization (EM) algorithm is used to find the parameters that maximize the likelihood of the data.\n",
    "- GMMs offer flexibility in capturing elliptical shapes and overlapping clusters and can be used for anomaly detection or probabilistic classification. Because of their reliance on probability estimation, GMMs can fluidly assign data to clusters depending on thresholds, if needed.\n",
    "\n",
    "## There’s more...\n",
    "- Visualizing cluster probabilities with GMM provides a predict_proba method to show the likelihood of each point belonging to each cluster. This can be useful when uncertainty about data point cluster assignments come into play.\n",
    "- Also, one can apply Dimensionality reduction with advanced clustering. Advanced clustering methods like Spectral Clustering can benefit from prior dimensionality reduction (e.g., PCA or t-SNE), especially with high-dimensional data. This step enhances performance and interpretability.\n",
    "\n",
    "# Practical Exercises with Clustering Models\n",
    "In this final section, readers will engage in practical exercises that involve building, tuning, and evaluating clustering models on real-world datasets. These exercises are designed to reinforce the concepts learned throughout the chapter and demonstrate how to effectively apply clustering techniques in various scenarios. By the end of this section, readers will have hands-on experience that they can leverage in their own ML projects.\n",
    "\n",
    "## Exercise 1: Clustering with K-Means on the Iris Dataset\n",
    "In this example, we’ll apply K-Means clustering to the well-known Iris dataset and evaluate the results using multiple metrics.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Load the Dataset\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Create and Train the KMeans Model\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Evaluate the Clustering\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Visualize the Cluster Assignments (PCA Projection)\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Comparing DBSCAN and K-Means on Moons Data\n",
    "This exercise demonstrates how DBSCAN can outperform K-Means on data with non-convex shapes.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Create and Scale the Dataset\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Apply KMeans and DBSCAN\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Visualize the Clustering Results\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Clustering High-Dimensional Data with PCA + GMM\n",
    "This exercise combines dimensionality reduction with a probabilistic clustering approach using Gaussian Mixture Models.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Load and Preprocess the Dataset\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Apply PCA for Dimensionality Reduction\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Fit Gaussian Mixture Model\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Visualize the Clustered Output\n",
    "YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
