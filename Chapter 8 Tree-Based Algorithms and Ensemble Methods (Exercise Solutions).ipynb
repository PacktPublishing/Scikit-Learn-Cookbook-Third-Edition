{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises with Tree-Based Models (Exercise Solutions)\n",
    "In this final section, we will engage in practical exercises that involve building, tuning, and evaluating tree-based and ensemble models on real-world datasets. These exercises are designed to reinforce the concepts learned throughout the chapter and demonstrate how to effectively apply these models in complex machine learning scenarios. By the end of this section, we will have hands-on experience that we can leverage in our own ML projects.\n",
    "\n",
    "### Exercise 1: Building and Evaluating a Decision Tree Classifier\n",
    "In this exercise, we'll build and evaluate a basic decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)\n",
    "\n",
    "# Create and train the classifier\n",
    "clf = DecisionTreeClassifier(random_state=2024)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#Evaluate performance\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyperparameter Tuning with Random Forests\n",
    "We'll fine-tune a random forest classifier using grid search to find the optimal parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)\n",
    "\n",
    "# Define hyperparameter grid and perform grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'max_features': ['sqrt', 'log2'] \n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=2024), param_grid, cv=5, error_score='raise')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Comparing Gradient Boosting and Random Forest\n",
    "We'll compare the performance of gradient boosting and random forest classifiers on a classification dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)\n",
    "\n",
    "# Create and train models\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=2024)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=2024)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "print(f'Random Forest Accuracy: {rf_accuracy:.2f}')\n",
    "print(f'Gradient Boosting Accuracy: {gb_accuracy:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
