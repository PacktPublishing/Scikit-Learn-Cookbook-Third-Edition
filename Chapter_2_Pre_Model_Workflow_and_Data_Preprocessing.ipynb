{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Pre-Model Workflow and Data Preprocessing\n",
    "\n",
    "This notebook provides \"recipes\" for using the scikit-learn Python library to preprocess data before modeling. Each recipe includes explanations, code examples, visualizations, best practices, and common pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 1: Handling Missing Data\n",
    "\n",
    "In this section, we will explore different strategies for handling missing data using scikit-learn's imputation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer\n",
    "\n",
    "The `SimpleImputer` class provides basic strategies for imputing missing values. It can replace missing values using a constant, the mean, median, or most frequent value of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset with missing values\n",
    "data = {\"Feature1\": [1, 2, np.nan, 4], \"Feature2\": [np.nan, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(df)\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=df.columns)\n",
    "imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNNImputer\n",
    "\n",
    "The `KNNImputer` class uses the k-Nearest Neighbors approach to impute missing values. It considers the nearest neighbors to estimate the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Initialize the KNNImputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "knn_imputed_data = knn_imputer.fit_transform(df)\n",
    "knn_imputed_df = pd.DataFrame(knn_imputed_data, columns=df.columns)\n",
    "knn_imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IterativeImputer\n",
    "\n",
    "The `IterativeImputer` class models each feature with missing values as a function of other features, and iteratively estimates missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Initialize the IterativeImputer\n",
    "iterative_imputer = IterativeImputer()\n",
    "\n",
    "# Fit and transform the data\n",
    "iterative_imputed_data = iterative_imputer.fit_transform(df)\n",
    "iterative_imputed_df = pd.DataFrame(iterative_imputed_data, columns=df.columns)\n",
    "iterative_imputed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations Comparing Strategies\n",
    "\n",
    "Visualize the differences in imputation strategies using bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the imputed data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "imputed_df.plot(kind=\"bar\", ax=axes[0], title=\"SimpleImputer\")\n",
    "knn_imputed_df.plot(kind=\"bar\", ax=axes[1], title=\"KNNImputer\")\n",
    "iterative_imputed_df.plot(kind=\"bar\", ax=axes[2], title=\"IterativeImputer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 2: Scaling and Normalization\n",
    "\n",
    "Scaling and normalization are crucial steps in preprocessing data for machine learning models. They ensure that each feature contributes equally to the distance calculations in algorithms like k-NN and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "The `StandardScaler` standardizes features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "The `MinMaxScaler` transforms features by scaling each feature to a given range, often between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "minmax_scaled_data = minmax_scaler.fit_transform(df)\n",
    "minmax_scaled_df = pd.DataFrame(minmax_scaled_data, columns=df.columns)\n",
    "minmax_scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer\n",
    "\n",
    "The `Normalizer` scales individual samples to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Initialize the Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data = normalizer.fit_transform(df)\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparisons\n",
    "\n",
    "Visualize the effects of different scaling and normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scaled data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "scaled_df.plot(kind=\"bar\", ax=axes[0], title=\"StandardScaler\")\n",
    "minmax_scaled_df.plot(kind=\"bar\", ax=axes[1], title=\"MinMaxScaler\")\n",
    "normalized_df.plot(kind=\"bar\", ax=axes[2], title=\"Normalizer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 3: Encoding Categorical Variables\n",
    "\n",
    "Encoding categorical variables is essential for converting non-numeric data into a format that can be used by machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoder\n",
    "\n",
    "The `OneHotEncoder` converts categorical values into a one-hot numeric array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "categorical_data = pd.DataFrame({\"Category\": [\"A\", \"B\", \"A\", \"C\"]})\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "onehot_encoded_data = onehot_encoder.fit_transform(categorical_data)\n",
    "onehot_encoded_df = pd.DataFrame(\n",
    "    onehot_encoded_data, columns=onehot_encoder.get_feature_names_out()\n",
    ")\n",
    "onehot_encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder\n",
    "\n",
    "The `LabelEncoder` encodes target labels with values between 0 and n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "label_encoded_data = label_encoder.fit_transform(categorical_data[\"Category\"])\n",
    "label_encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColumnTransformer\n",
    "\n",
    "The `ColumnTransformer` allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample mixed data\n",
    "mixed_data = pd.DataFrame({\"Numeric\": [1, 2, 3, 4], \"Category\": [\"A\", \"B\", \"A\", \"C\"]})\n",
    "\n",
    "# Initialize the ColumnTransformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    [(\"num\", StandardScaler(), [\"Numeric\"]), (\"cat\", OneHotEncoder(), [\"Category\"])]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "transformed_data = column_transformer.fit_transform(mixed_data)\n",
    "transformed_df = pd.DataFrame(\n",
    "    transformed_data,\n",
    "    columns=[\"Numeric_scaled\"] + list(onehot_encoder.get_feature_names_out()),\n",
    ")\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding Considerations\n",
    "\n",
    "Target encoding is a technique where categorical variables are replaced with the mean of the target variable. It can be useful but also risky due to potential data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 4: Introduction to Pipelines\n",
    "\n",
    "Pipelines are a simple way to streamline a machine learning workflow by chaining together transformers and estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Pipeline Construction\n",
    "\n",
    "A basic pipeline chains together a sequence of transformations and a final estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "predictions = pipeline.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline with Multiple Steps\n",
    "\n",
    "A more complex pipeline can include multiple preprocessing steps before the final estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex pipeline\n",
    "complex_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"classifier\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "complex_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "complex_predictions = complex_pipeline.predict(X_test)\n",
    "complex_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Visualization\n",
    "\n",
    "Visualizing pipelines can help understand the workflow and ensure all steps are correctly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "# Set display configuration\n",
    "set_config(display=\"diagram\")\n",
    "\n",
    "# Display the pipeline\n",
    "complex_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 5: Feature Engineering\n",
    "\n",
    "Feature engineering involves creating new features or modifying existing ones to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features\n",
    "\n",
    "The `PolynomialFeatures` transformer generates polynomial and interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Initialize the PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "poly_features = poly.fit_transform(df)\n",
    "poly_features_df = pd.DataFrame(\n",
    "    poly_features, columns=poly.get_feature_names_out(df.columns)\n",
    ")\n",
    "poly_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning with KBinsDiscretizer\n",
    "\n",
    "The `KBinsDiscretizer` discretizes continuous features into k bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Initialize the KBinsDiscretizer\n",
    "kbins = KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\")\n",
    "\n",
    "# Fit and transform the data\n",
    "binned_data = kbins.fit_transform(df)\n",
    "binned_df = pd.DataFrame(binned_data, columns=df.columns)\n",
    "binned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Feature selection helps in selecting the most important features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the RFE\n",
    "rfe = RFE(estimator=LinearRegression(), n_features_to_select=1)\n",
    "\n",
    "# Fit the RFE\n",
    "rfe.fit(df, y)\n",
    "\n",
    "# Get the ranking of features\n",
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 6: Practical Exercises\n",
    "\n",
    "In this section, we will combine all the recipes into a comprehensive pipeline and apply it to the California Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Pipeline\n",
    "\n",
    "We will create a pipeline that includes imputation, scaling, encoding, and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_data = fetch_california_housing()\n",
    "X, y = california_data.data, california_data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a comprehensive pipeline\n",
    "comprehensive_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", RandomForestRegressor()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "comprehensive_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "score = comprehensive_pipeline.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "\n",
    "Evaluate the performance of the comprehensive pipeline on the California Housing dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
