{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Linear Models and Regularization\n",
    "\n",
    "This notebook provides practical \"recipes\" for linear regression and regularization tasks in scikit-learn. Each recipe includes explanations, code examples, visualizations, best practices, and common pitfalls. We'll cover implementation, tuning, and evaluation of linear regression and regularization models.\n",
    "\n",
    "## Introduction to Linear Models\n",
    "\n",
    "### Getting ready\n",
    "To get started, you'll notice that we're taking a familiar approach used in previous chapters by creating a synthetic dataset to demonstrate the concepts of linear models and regularization. scikit-learn provides a function called make_regression() that creates a synthetic regression dataset which we can further refine to create a more interesting dataset that includes some \"noise\" to make it more challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create synthetic regression dataset with high multicollinearity and more features\n",
    "X, y = make_regression(n_samples=1000,\n",
    "                      n_features=100,     \n",
    "                      n_informative=10,   \n",
    "                      noise=20,           \n",
    "                      random_state=123)\n",
    "\n",
    "# Add multicollinearity by creating correlated features\n",
    "for i in range(50, 100):  # Make half the features correlated\n",
    "    X[:, i] = X[:, i-50] + np.random.normal(0, 0.1, size=1000)\n",
    "\n",
    "# Create feature names\n",
    "feature_names = [f'feature_{i}' for i in range(100)]\n",
    "\n",
    "# Take just the first feature for visualization\n",
    "X_plot = X[:, 0].reshape(-1, 1)\n",
    "\n",
    "# Add some non-linearity and make coefficients vary widely in magnitude\n",
    "y = y * 1000  # Scale up the target\n",
    "y = y + np.sin(X_plot.ravel()) * 150 + np.exp(X_plot.ravel()/10)\n",
    "\n",
    "# Convert to pandas DataFrame with feature names\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "df_plot = pd.DataFrame({'feature_0': X_plot.ravel(), 'target': y})\n",
    "\n",
    "# Quick visualization of the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_plot, y, alpha=0.5)\n",
    "plt.xlabel('feature_0')\n",
    "plt.ylabel('target')\n",
    "plt.title('Synthetic Regression Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it\n",
    "Implementation of a linear regression model is straightforward. We'll use the train_test_split() function to split the data into training and testing sets, and then fit a linear regression model to the training data. Finally, we'll evaluate the model's performance using mean squared error (MSE) and R-squared (R^2) metrics. We can also visualize the model's predictions on the test set to see how well it fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Create and fit Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance using MSE and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "\n",
    "# Visualize the data and regression line (using feature_0 for visualization)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test['feature_0'], y_test, color='blue', alpha=0.5, label='Test Data')\n",
    "plt.scatter(X_train['feature_0'], y_train, color='green', alpha=0.5, label='Train Data')\n",
    "\n",
    "# Sort the data for a smooth line plot\n",
    "X_line = np.linspace(df['feature_0'].min(), df['feature_0'].max(), 100).reshape(-1, 1)\n",
    "X_line_full = np.zeros((100, len(feature_names)))\n",
    "X_line_full[:, 0] = X_line.ravel()  # Set feature_0, leave others as 0\n",
    "y_line = linear_model.predict(pd.DataFrame(X_line_full, columns=feature_names))\n",
    "\n",
    "plt.plot(X_line, y_line, color='red', label='Regression Line')\n",
    "plt.xlabel('feature_0')\n",
    "plt.ylabel('target')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "Linear regression is a fundamental technique used to model relationships between a dependent variable (target) and one or more independent variables (features). The model assumes that there is a linear relationship between these variables\n",
    "\n",
    "## Ridge and Lasso Regression\n",
    "\n",
    "### Getting ready\n",
    "In order to implement Ridge and Lasso regression, we'll use the Ridge and Lasso classes from the sklearn.linear_model module. These classes are similar to `LinearRegression()`, but they include an additional parameter called alpha, which controls the strength of the regularization. We can use the same dataset created in the previous section to demonstrate the implementation of Ridge and Lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it\n",
    "Let's fit both a Ridge and Lasso regression model to the data and compare the results with the linear regression model we created in the previous section. You'll notice that the Ridge and Lasso models perform better than the linear regression model, but the difference is not as significant as you might expect. This is because the data is highly multicollinear and the features are highly correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and fit Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)  # alpha controls regularization strength\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and fit Lasso regression model with increased iterations and alpha\n",
    "lasso_model = Lasso(alpha=10.0, max_iter=10000, tol=0.001)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with both models\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for all models\n",
    "metrics = {\n",
    "    'Model': ['Ridge Regression', 'Lasso Regression', 'Linear Regression'],\n",
    "    'Mean Squared Error': [\n",
    "        mean_squared_error(y_test, y_pred_ridge),\n",
    "        mean_squared_error(y_test, y_pred_lasso),\n",
    "        mean_squared_error(y_test, y_pred_linear)\n",
    "    ],\n",
    "    'R-squared': [\n",
    "        r2_score(y_test, y_pred_ridge),\n",
    "        r2_score(y_test, y_pred_lasso), \n",
    "        r2_score(y_test, y_pred_linear)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame and sort by MSE\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df = metrics_df.sort_values('Mean Squared Error', ascending=True)\n",
    "\n",
    "# Format the numeric columns\n",
    "metrics_df['Mean Squared Error'] = metrics_df['Mean Squared Error'].map('{:.2f}'.format)\n",
    "metrics_df['R-squared'] = metrics_df['R-squared'].map('{:.2f}'.format)\n",
    "\n",
    "display(metrics_df)\n",
    "\n",
    "# Visualize the results (using feature_0 for visualization)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test['feature_0'], y_test, color='blue', alpha=0.5, label='Test Data')\n",
    "\n",
    "# Sort the data for smooth line plots\n",
    "X_line = np.linspace(df['feature_0'].min(), df['feature_0'].max(), 100).reshape(-1, 1)\n",
    "X_line_full = np.zeros((100, len(feature_names)))\n",
    "X_line_full[:, 0] = X_line.ravel()\n",
    "X_line_df = pd.DataFrame(X_line_full, columns=feature_names)\n",
    "\n",
    "# Generate predictions for the line\n",
    "y_line_ridge = ridge_model.predict(X_line_df)\n",
    "y_line_lasso = lasso_model.predict(X_line_df)\n",
    "y_line_linear = linear_model.predict(X_line_df)\n",
    "\n",
    "plt.plot(X_line, y_line_ridge, color='red', label='Ridge Regression')\n",
    "plt.plot(X_line, y_line_lasso, color='green', linestyle='--', label='Lasso Regression')\n",
    "plt.plot(X_line, y_line_linear, color='purple', label='Linear Regression')\n",
    "plt.xlabel('feature_0')\n",
    "plt.ylabel('target')\n",
    "plt.title('Ridge, Lasso, and Linear Regression Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet and Regularization \n",
    "\n",
    "### Getting ready\n",
    "In order to implement ElasticNet regression, we'll use the ElasticNet class from the sklearn.linear_model module. This class is similar to Ridge and Lasso regression, but it combines their strengths to handle different types of data. We can use the same dataset created in the previous section to demonstrate the implementation of ElasticNet regression. \n",
    "\n",
    "### How to do it\n",
    "We will create a range of `alpha` and `l1_ratio` values to test and plot the coefficient paths for each combination. This is the main purpose of ElasticNet, to handle the multicollinearity and high-dimensional data that is common in real-world datasets. The resulting visualization is called a \"coefficient path plot\" and it shows the relationship between the alpha and the coefficient for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create a range of alphas to test\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "l1_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# Store coefficients for plotting\n",
    "coef_paths = []\n",
    "labels = []\n",
    "\n",
    "# Create and fit ElasticNet models with different parameters\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for l1_ratio in l1_ratios:\n",
    "    coefs = []\n",
    "    for alpha in alphas:\n",
    "        # Create and fit ElasticNet model with increased max_iter and tol\n",
    "        elastic_model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=123,\n",
    "                                 max_iter=10000, tol=1e-4)\n",
    "        elastic_model.fit(X_train, y_train)\n",
    "        coefs.append(elastic_model.coef_)\n",
    "    \n",
    "    # Convert coefficients to array and store\n",
    "    coef_paths.append(np.array(coefs))\n",
    "    \n",
    "    # Plot coefficient paths\n",
    "    for feature_idx in range(X_train.shape[1]):\n",
    "        plt.plot(np.log10(alphas), \n",
    "                np.array(coefs)[:, feature_idx], \n",
    "                label=f'l1_ratio={l1_ratio}, feature_{feature_idx}' if feature_idx == 0 else \"\",\n",
    "                alpha=0.5,\n",
    "                linestyle=['solid', 'dashed', 'dotted', 'dashdot', '-'][l1_ratios.index(l1_ratio)])\n",
    "\n",
    "plt.xlabel('log(alpha)')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.title('ElasticNet Coefficient Paths')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a model with balanced l1_ratio and moderate alpha\n",
    "elastic_model = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=123,\n",
    "                          max_iter=10000, tol=1e-4)  \n",
    "elastic_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate metrics\n",
    "elastic_pred = elastic_model.predict(X_test)\n",
    "elastic_mse = mean_squared_error(y_test, elastic_pred)\n",
    "elastic_r2 = r2_score(y_test, elastic_pred)\n",
    "\n",
    "# Add ElasticNet results to metrics DataFrame\n",
    "new_row = pd.DataFrame({\n",
    "    'Model': ['ElasticNet Regression'],\n",
    "    'Mean Squared Error': [elastic_mse],\n",
    "    'R-squared': [elastic_r2]\n",
    "})\n",
    "\n",
    "# Remove any existing ElasticNet entries before concatenating\n",
    "metrics_df = metrics_df[~metrics_df['Model'].str.contains('ElasticNet')]\n",
    "\n",
    "# Convert string values back to float for sorting\n",
    "metrics_df['Mean Squared Error'] = metrics_df['Mean Squared Error'].astype(float)\n",
    "metrics_df['R-squared'] = metrics_df['R-squared'].astype(float)\n",
    "\n",
    "metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "\n",
    "# Sort the metrics DataFrame by Mean Squared Error\n",
    "metrics_df = metrics_df.sort_values('Mean Squared Error', ascending=True)\n",
    "\n",
    "# Format the numeric columns for display\n",
    "metrics_df['Mean Squared Error'] = metrics_df['Mean Squared Error'].map('{:.2f}'.format)\n",
    "metrics_df['R-squared'] = metrics_df['R-squared'].map('{:.2f}'.format)\n",
    "\n",
    "# Display updated metrics\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression \n",
    "\n",
    "### Getting ready\n",
    "Let’s move on and explore polynomial regression (including spline interpolation). These methods extend the capabilities of traditional linear regression, allowing for more flexible modeling of complex relationships in data. By implementing these techniques, you can expand your toolbox of approaches to regression problems.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create synthetic dataset with non-linear relationships\n",
    "np.random.seed(123)\n",
    "n_samples = 1000\n",
    "X = np.random.uniform(-50, 50, (n_samples, 1))\n",
    "\n",
    "# Create target with non-linear components:\n",
    "y = (2 * X[:, 0]  # linear component\n",
    "     + 27 * np.sin(X[:, 0] / 8)  \n",
    "     + np.random.normal(0, 4, n_samples))  # noise\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame(X, columns=['feature1'])\n",
    "data['target'] = y\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.scatterplot(data=data, x='feature1', y='target')\n",
    "plt.title('Feature 1 vs Target')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it...\n",
    "In the code below, we will create a list of polynomial degrees to try and then fit a linear regression model on the polynomial features. We will then evaluate the model's performance using mean squared error (MSE) and R-squared (R^2) metrics. Finally, we will visualize the different polynomial fits and the residuals for the best performing model. Keep in mind that the best performing model is not always the one with the highest R-squared value, but rather the one that best fits the data and has the lowest mean squared error. Also, the visualization is a bit messy, but it's a good starting point (you probably won't come across a dataset with such a clean relationship between the features and the target with a perfect sine wave nonlinearity, anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the synthetic dataset created above\n",
    "X = data[['feature1']]  \n",
    "y = data['target']\n",
    "\n",
    "# Create a list of polynomial degrees to try\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "results = []\n",
    "\n",
    "# Try different polynomial degrees\n",
    "for degree in degrees:\n",
    "    # Transform features to polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=123)\n",
    "    \n",
    "    # Create and fit Linear Regression model on polynomial features\n",
    "    poly_model = LinearRegression()\n",
    "    poly_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = poly_model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Degree': degree,\n",
    "        'MSE': mse,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "# Create DataFrame of results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Create visualization of different polynomial fits\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot data and polynomial fits\n",
    "ax1.scatter(X['feature1'], y, alpha=0.2, label='Data points')\n",
    "\n",
    "# Generate points for smooth curve plotting\n",
    "X_plot = np.linspace(X['feature1'].min(), X['feature1'].max(), 1000).reshape(-1, 1)\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'purple', 'black']\n",
    "for degree, color in zip(degrees, colors):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_plot_poly = poly.fit_transform(X_plot)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(poly.fit_transform(X), y)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "    \n",
    "    ax1.plot(X_plot, y_plot, label=f'Degree {degree}', color=color, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Target')\n",
    "ax1.set_title('Polynomial Regression Fits')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot residuals for best performing model (based on R2)\n",
    "best_degree = results_df.loc[results_df['R2'].idxmax(), 'Degree']\n",
    "best_poly = PolynomialFeatures(degree=best_degree)\n",
    "X_poly_best = best_poly.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_best, y, test_size=0.2, random_state=123)\n",
    "\n",
    "best_model = LinearRegression()\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "ax2.scatter(y_pred, residuals, alpha=0.5)\n",
    "ax2.axhline(y=0, color='r', linestyle='--')\n",
    "ax2.set_xlabel('Predicted Values')\n",
    "ax2.set_ylabel('Residuals')\n",
    "ax2.set_title(f'Residual Plot (Best Model: Degree {best_degree})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spline Interpolation \n",
    "Spline interpolation is a technique used to create a smooth curve that passes through a set of data points. It is a type of interpolation that uses piecewise polynomials to approximate the data. The basic idea is to divide the data into segments, and then fit a polynomial to each segment. The resulting curve is smooth and continuous at the points where the segments meet.\n",
    "\n",
    "scikit-learn provides a SplineTransformer class that can be used to create spline interpolation. This class allows you to specify the number of \"knots\" (the points where the segments meet) and the degree of the polynomial to use for each segment. The SplineTransformer class is part of the sklearn.preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Create a spline transformer with different degrees of freedom\n",
    "n_knots = [3, 5, 7]  # Different numbers of knots to try\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create lists to store metrics\n",
    "mse_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for n_knot, color in zip(n_knots, colors[:len(n_knots)]):\n",
    "    # Create and fit spline transformer\n",
    "    spline = SplineTransformer(n_knots=n_knot, degree=3)\n",
    "    model = make_pipeline(spline, LinearRegression())\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred = model.predict(X_plot)\n",
    "    y_pred_train = model.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y, y_pred_train)\n",
    "    r2 = r2_score(y, y_pred_train)\n",
    "    mse_scores.append(mse)\n",
    "    r2_scores.append(r2)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.plot(X_plot, y_pred, \n",
    "             label=f'Spline (knots={n_knot})', \n",
    "             color=color, \n",
    "             linewidth=2)\n",
    "\n",
    "# Plot original data points\n",
    "plt.scatter(X, y, color='black', alpha=0.5, label='Data points')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Spline Interpolation with Different Numbers of Knots')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Create and display metrics DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Number of Knots': n_knots,\n",
    "    'MSE': mse_scores,\n",
    "    'R-squared': r2_scores\n",
    "})\n",
    "display(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises with Linear Models and Regularization\n",
    "\n",
    "### Exercise 1: Implementing Ridge Regression\n",
    "In the first execise, we will create a new dataset and fit a Ridge regression model to it. We will then evaluate the model's performance using mean squared error (MSE) and R-squared (R^2) metrics. Finally, we will visualize the model's predictions on the test set to see how well it fits the data. When applicable, be sure to use random_state=123 and/or np.random.seed(123) to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Generate synthetic dataset with some noise\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Split data into training and test sets\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Create and fit Ridge regression model\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Make predictions\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Calculate metrics\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Print metrics\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Visualize results\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Sort X values for smooth line plot\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implementing Lasso Regression\n",
    "In this next exercise, we will create a new dataset and fit a Lasso regression model to it. We will then evaluate the model's performance using mean squared error (MSE) and R-squared (R^2) metrics. Finally, we will visualize the model's predictions on the test set to see how well it fits the data. When applicable, be sure to use random_state=123 and/or np.random.seed(123) to ensure reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Generate synthetic data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Split the data into training and test sets\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Create and fit Lasso regression model\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Make predictions\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Calculate metrics\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Print metrics\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Visualize results\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Sort X values for smooth line plot\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implementing ElasticNet Regression\n",
    "In this final exercise, we will create a new dataset and fit an ElasticNet regression model to it. We will then evaluate the model's performance using mean squared error (MSE) and R-squared (R^2) metrics. Finally, we will visualize the model's predictions on the test set to see how well it fits the data. When applicable, be sure to use random_state=123 and/or np.random.seed(123) to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Generate synthetic data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Split the data into training and test sets\n",
    "YOUR CODE HERE  \n",
    "\n",
    "# Create and fit ElasticNet model\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Make predictions\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Calculate metrics\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Print metrics\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Visualize results\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Sort X values for smooth line plot\n",
    "YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
