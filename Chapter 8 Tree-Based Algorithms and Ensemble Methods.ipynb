{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Support Vector Machines and Kernel Methods\n",
    "\n",
    "This notebook provides practical \"recipes\" for tree-base algorithms and  ensemble methods. Each recipe includes explanations, code examples, visualizations, best practices, and common pitfalls when applicable. \n",
    "\n",
    "## Introduction to Decision Trees\n",
    "Decision trees are powerful and, compared to other ML techniques, intuitive models used for classification and regression tasks. They work by recursively splitting data based on feature values, creating a tree-like structure composed of nodes and branches. Each internal node represents a “decision” based on a feature, while “leaf nodes” represent the predicted outcome. Decision trees are popular due to their interpretability and effectiveness in handling both numerical and categorical data. Even though they are relatively easy to understand, they are still powerful and can often outperform more complex models, so don’t dismiss them from your ML arsenal!.\n",
    "\n",
    "### Getting ready\n",
    "First, we need to prepare our environment and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it\n",
    "We can now build our decision tree classifier, train it, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "clf = DecisionTreeClassifier(random_state=2024)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Create a classification report\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Stylize the DataFrame\n",
    "styled_df = (report_df\n",
    "    .style\n",
    "    .background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score'])\n",
    "    .format({\n",
    "        'precision': '{:.3f}',\n",
    "        'recall': '{:.3f}', \n",
    "        'f1-score': '{:.3f}',\n",
    "        'support': '{:.0f}'\n",
    "    })\n",
    ")\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works\n",
    "Decision trees work by splitting the dataset based on feature values. The goal at each split is to increase homogeneity, or “sameness,” within each subgroup. The decision to split is typically determined by criteria like Gini impurity (used by default in scikit-learn) or entropy.\n",
    "We can visualize the decision tree to better understand how splits were made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the figure\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, proportion=True, rounded=True, fontsize=10)\n",
    "plt.title(\"Decision Tree for Iris Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests and Bagging\n",
    "While building a single decision tree model is intuitive, most real-world applications will only use them as part of an ensemble method due to some of their shortcomings – especially in overfitting. As the saying goes, “two heads (or trees in this case) are better than one!” \n",
    "\n",
    "### Getting ready\n",
    "We will utilize scikit-learn to demonstrate building a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it\n",
    "We will now build, train, and evaluate our random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=2024)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Stylize the DataFrame\n",
    "styled_df = (report_df\n",
    "    .style\n",
    "    .background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score'])\n",
    "    .format({\n",
    "        'precision': '{:.3f}',\n",
    "        'recall': '{:.3f}', \n",
    "        'f1-score': '{:.3f}',\n",
    "        'support': '{:.0f}'\n",
    "    })\n",
    ")\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests construct multiple decision trees, each trained on “bootstrapped” samples of the dataset. Additionally, at each “split” in a tree, a random subset of features is considered, introducing further diversity. This approach reduces variance and increases model stability and generalization which is designed to allow it to work with unseen data it might encounter in a production application.\n",
    "\n",
    "Let's visualize feature importance to understand which features contributed most to the decision-making process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate feature importance\n",
    "importances = rf_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Feature Importances in Random Forest\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices], rotation=45)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Machines\n",
    "Gradient boosting machines (GBMs) are advanced ensemble techniques that sequentially build and combine “weak” prediction models, typically decision trees, to produce a stronger predictive performance. Unlike random forests, GBMs construct trees one at a time, each aiming to minimize errors from previous models. Another way to think about this is while random forests build a collection of decision trees in parallel, GBMs build them sequentially. This is where the term “boosting” comes from: we try to “boost” the predictive performance of each successive tree. This iterative approach can significantly enhance accuracy, making GBMs highly effective for various machine learning tasks.\n",
    "\n",
    "### Getting ready\n",
    "We will use scikit-learn to illustrate how to create a gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\t\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it...\n",
    "Now, let's build and evaluate our gradient boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "gbm_clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.2, random_state=2024)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gbm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = gbm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "\n",
    "# Stylize the DataFrame\n",
    "styled_df = (report\n",
    "    .style\n",
    "    .background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score'])\n",
    "    .format({\n",
    "        'precision': '{:.3f}',\n",
    "        'recall': '{:.3f}', \n",
    "        'f1-score': '{:.3f}',\n",
    "        'support': '{:.0f}'\n",
    "    })\n",
    ")\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works...\n",
    "GBMs work by iteratively building new models that predict the residuals (errors) of previous models. Each new model aims to minimize these residuals, gradually refining the overall prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Trees and Ensembles\n",
    "As we’ve well-learned by now, hyperparameter tuning is essential for optimizing the performance of models and ensembles, including decision trees, random forests, and GBMs. By carefully selecting hyperparameters such as maximum tree depth, number of estimators, and learning rates, we can significantly enhance model performance and prevent overfitting. We will utilize the same tools (only the hyperparameters themselves will be specific to these model types) we used previously in scikit-learn, like grid search and cross-validation, to systematically tune our models.\n",
    "\n",
    "### Getting ready\n",
    "We'll demonstrate hyperparameter tuning using scikit-learn's GridSearchCV() with a gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it...\n",
    "We'll now use grid search combined with cross-validation to find the best hyperparameters for our gradient boosting model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV()\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=2024),\n",
    "                           param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV() to training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Identify the best parameters\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Generate classification report\n",
    "report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()\n",
    "\n",
    "# Stylize the DataFrame\n",
    "styled_df = (report\n",
    "    .style\n",
    "    .background_gradient(cmap='Blues', subset=['precision', 'recall', 'f1-score'])\n",
    "    .format({\n",
    "        'precision': '{:.3f}',\n",
    "        'recall': '{:.3f}', \n",
    "        'f1-score': '{:.3f}',\n",
    "        'support': '{:.0f}'\n",
    "    })\n",
    ")\n",
    "\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Grid search exhaustively tests combinations of specified hyperparameters, using cross-validation to evaluate each set's performance. The model achieving the best cross-validation score is then selected as the “optimal” configuration. Cross-validation ensures reliable performance estimates by evaluating each hyperparameter configuration across multiple subsets of the dataset which is our way of trying to simulate the nuances our model might “see” when performing “in the wild.”\n",
    "\n",
    "We can visualize the results to better understand the effect of different hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a pivot table\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "pivot_table = results.pivot_table(values='mean_test_score', \n",
    "                                  index='param_max_depth', \n",
    "                                  columns='param_n_estimators')\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='viridis')\n",
    "plt.title('Grid Search Accuracy by Max Depth and Number of Estimators')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Max Depth')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Ensemble Methods\n",
    "Comparing ensemble methods helps us recognize the relative strengths and weaknesses of approaches like bagging, boosting, and stacking (which we’ll look at below). Each method has unique characteristics—bagging reduces variance, boosting reduces bias, and stacking leverages multiple algorithms to enhance predictive performance. Through comparative experiments on various datasets, we can determine which ensemble strategy works best for specific problems.\n",
    "\n",
    "### Getting ready\n",
    "We'll demonstrate comparing different ensemble methods using scikit-learn with a classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it...\n",
    "We'll build and evaluate bagging (random forests), boosting (gradient boosting), and stacking classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=2024)\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=2024)\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[('rf', rf_clf), ('gb', gb_clf)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "# Fit the models\n",
    "rf_clf.fit(X_train, y_train)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "stacking_pred = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "gb_accuracy = accuracy_score(y_test, gb_pred)\n",
    "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print(f'Random Forest Accuracy: {rf_accuracy:.2f}')\n",
    "print(f'Gradient Boosting Accuracy: {gb_accuracy:.2f}')\n",
    "print(f'Stacking Accuracy: {stacking_accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works...\n",
    "Bagging (with random forests) averages predictions from multiple trees built on bootstrapped datasets, reducing variance. Boosting (with GBMs) sequentially builds trees to correct errors from previous models, focusing on reducing bias. Stacking, on the other hand, combines multiple base models using a “meta-model” to enhance predictive performance further.\n",
    "\n",
    "We can visualize the comparative results clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar plot\n",
    "methods = ['Random Forest', 'Gradient Boosting', 'Stacking']\n",
    "accuracies = [rf_accuracy, gb_accuracy, stacking_accuracy]\n",
    "\n",
    "# Plot the bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(methods, accuracies, color=['skyblue', 'salmon', 'lightgreen'])\n",
    "plt.title('Accuracy Comparison of Ensemble Methods')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercises with Tree-Based Models\n",
    "In this final section, we will engage in practical exercises that involve building, tuning, and evaluating tree-based and ensemble models on real-world datasets. These exercises are designed to reinforce the concepts learned throughout the chapter and demonstrate how to effectively apply these models in complex machine learning scenarios. By the end of this section, we will have hands-on experience that we can leverage in our own ML projects.\n",
    "\n",
    "### Exercise 1: Building and Evaluating a Decision Tree Classifier\n",
    "In this exercise, we'll build and evaluate a basic decision tree classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Load the dataset\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Split the data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Create and train the classifier\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Make predictions\n",
    "YOUR CODE HERE\n",
    "\n",
    "#Evaluate performance\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyperparameter Tuning with Random Forests\n",
    "We'll fine-tune a random forest classifier using grid search to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Load the dataset\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Split the data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Define hyperparameter grid and perform grid search\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Evaluate the best model\n",
    "YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Comparing Gradient Boosting and Random Forest\n",
    "We'll compare the performance of gradient boosting and random forest classifiers on a classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Load the dataset\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Split the data\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Create and train models\n",
    "YOUR CODE HERE\n",
    "\n",
    "# Make predictions and evaluate\n",
    "YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
