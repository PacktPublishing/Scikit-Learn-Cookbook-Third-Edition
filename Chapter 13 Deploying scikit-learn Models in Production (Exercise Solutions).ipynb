{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Deploying scikit-learn Models in Production\n",
    "This notebook provides practical \"recipes\" for cross-validation and model evaluation techniques. Each recipe includes explanations, code examples, visualizations, best practices, and common pitfalls when applicable. \n",
    "\n",
    "Deploying a model means moving it from your development environment into production, so that real users or systems can access its predictions. Deployment involves preparing a reliable artifact, or model metadata, serving it with appropriate latency and throughput, and ensuring that it continues to perform well as the data evolves. We’ll walk through the typical steps of packaging, exposing, and verifying a trained scikit-learn model in production-like conditions.\n",
    "\n",
    "## Getting ready\n",
    "Before deployment, ensure we have a trained scikit-learn model and necessary libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from joblib import dump, load\n",
    "\n",
    "# Create and train a simple model\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=2024)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We'll persist the model to disk and simulate loading it in a new environment (“persist” is just a way of saying we’ll save the ML model in an appropriate format for future use in a prediction setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model using joblib\n",
    "dump(clf, \"model.joblib\")\n",
    "\n",
    "# Load the model back\n",
    "clf_loaded = load(\"model.joblib\")\n",
    "\n",
    "# Make a prediction with the loaded model\n",
    "incoming = np.random.rand(1, 20)\n",
    "print(clf_loaded.predict(incoming))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "The joblib module is the recommended way to persist scikit-learn models; it handles NumPy arrays efficiently compared to standard pickle. Once exported to .joblib, the model can be uploaded to a production server and loaded by the same library versions. After loading, the model behaves identically to the original, allowing your client systems to fetch predictions reliably.\n",
    "\n",
    "# Serialization and Persistence Techniques\n",
    "Saving and reloading models is essential for production workflows—training usually happens once in a given CI/CD/CT cycle, but prediction must happen repeatedly. In this section, we’ll demonstrate how to serialize models with both `pickle` and `joblib`, discuss security and versioning considerations, and show how third party formats like ONNX may be used for Python free environments.\n",
    "\n",
    "## Getting ready\n",
    "You’ll need libraries to train a model and tools to persist it in multiple formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "# Train a classifier\n",
    "X, y = make_classification(n_samples=500, n_features=15, random_state=2024)\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=2024)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Applying serialization with both `pickle` and `joblib` is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize using pickle\n",
    "with open(\"rf.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf, f)\n",
    "\n",
    "# Serialize using joblib\n",
    "dump(rf, \"rf.joblib\")\n",
    "\n",
    "# Load the pickle model\n",
    "with open(\"rf.pkl\", \"rb\") as f:\n",
    "    rf1 = pickle.load(f)\n",
    "\n",
    "# Load the joblib model\n",
    "rf2 = load(\"rf.joblib\")\n",
    "\n",
    "# Ensure identical behavior after loading\n",
    "assert np.array_equal(rf.predict(X[:5]), rf1.predict(X[:5]))\n",
    "assert np.array_equal(rf.predict(X[:5]), rf2.predict(X[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Python’s pickle library works for serializing objects but can struggle with large NumPy arrays inside estimators. joblib.dump and joblib.load are optimized for such cases and are the recommended serialization method for scikit-learn models. Note, however, that trust is required when unpickling—never use untrusted files—and that models saved with one version of scikit-learn may break if loaded with a different version. For contexts where Python is not available, converters to ONNX or PMML may be used, although these formats may not support all scikit-learn estimators.\n",
    "\n",
    "# Scaling Models for Production\n",
    "When deploying models in real-world environments, you may encounter large datasets, distributed infrastructure, or high inference demand. Here, we’ll explore techniques to scale model training and prediction, including leveraging n_jobs, joblib parallelism, connecting to external backends like Dask (https://www.dask.org/), and designing for batch serving.\n",
    "\n",
    "## Getting ready\n",
    "You’ll need tools to run parallel inference and synthetic data to benchmark performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Train a forest model on synthetic data\n",
    "X, y = make_classification(n_samples=2000, n_features=50, random_state=2024)\n",
    "clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=2024)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Next, let’s test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure single-batch latency\n",
    "batch = np.random.rand(1000, 50)\n",
    "start = time.time()\n",
    "clf.predict(batch)\n",
    "print(\"Time:\", time.time() - start)\n",
    "\n",
    "# Train with parallel cross-validation\n",
    "scores = cross_val_score(RandomForestClassifier(n_estimators=50), X, y, cv=3, n_jobs=-1)\n",
    "print(\"CV Accuracy:\", np.mean(scores))\n",
    "\n",
    "# (Optional) Use Dask as a joblib backend if available\n",
    "# from dask.distributed import Client\n",
    "# from dask_ml.model_selection import GridSearchCV as daskGSCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "Many scikit-learn estimators support the n_jobs parameter for parallelization via joblib. For distributed environments, Dask can serve as an alternate joblib backend or support parallel meta-estimators for large workloads (basically, think of cloning your model several times across several compute/storage devices – like building an army of models). Batch serving—predicting large collections of inputs at once (i.e., in batches)—offers significantly better throughput than serving individual requests due to lower overhead and better CPU utilization.\n",
    "\n",
    "# Monitoring and Updating Deployed Models\n",
    "Once your model is live, its performance will degrade over time due to changes in input data distributions (\"drift\") or external conditions. This is basically a given in any scenario where an ML model is used and is not unexpected. This is where the CT in CI/CD/CT comes into play. You may be familiar with the term CI/CD, or Continuous Integration/Continuous Deployment or Deployment which is a software engineering paradigm for building and releasing software. However, in an ML deployment, we must also consider Continuous Training for monitoring and managing models as they degrade. This recipe guides you through strategies to monitor deployed models, detect drift, and update (retrain or adjust) models to maintain reliability.\n",
    "\n",
    "## Getting ready\n",
    "You’ll simulate incoming batches of data and require incremental-capable models to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Simulate initial training and streaming data\n",
    "X_train, y_train = make_classification(n_samples=500, n_features=10, random_state=2024)\n",
    "X_pub, y_pub = make_classification(n_samples=200, n_features=10, random_state=2025)\n",
    "stream_batches = np.array_split(X_pub, 4)\n",
    "stream_labels = np.array_split(y_pub, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Next, let’s train our model in batches to simulate data evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an incremental SGDClassifier on initial data\n",
    "clf = SGDClassifier(loss=\"log_loss\", random_state=2024, warm_start=True)\n",
    "clf.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
    "\n",
    "# Monitor each incoming batch\n",
    "batch_scores = []\n",
    "for xb, yb in zip(stream_batches, stream_labels):\n",
    "    yf = clf.predict(xb)\n",
    "    batch_scores.append(np.mean(yf == yb))\n",
    "\n",
    "# Trigger retraining when accuracy drops below threshold (e.g. < 0.8)\n",
    "if min(batch_scores) < 0.8:\n",
    "    clf.partial_fit(X_pub, y_pub)\n",
    "\n",
    "# Plot batch-by-batch accuracy\n",
    "plt.plot(batch_scores, marker=\"o\")\n",
    "plt.title(\"Batch Accuracy Over Time\")\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "For models that support partial_fit and warm_start, like SGDClassifier, you can continue training as new data arrives. Monitoring batch accuracy provides a simple drift detector. If performance drops below a defined threshold, retraining the model—or updating with streaming data—can restore accuracy. Visualizing accuracy trends enables easy interpretation of degradation.\n",
    "\n",
    "# Managing Model Lifecycle\n",
    "Managing a model’s lifecycle ensures it remains reliable, reproducible, and maintainable over time. In this recipe, we cover versioning, reproducibility measures, document control, and ensuring consistency across training and serving environments.\n",
    "\n",
    "## Getting ready\n",
    "You’ll prepare a model-saving script and tools to snapshot metadata and validation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump\n",
    "import json\n",
    "\n",
    "# Train a model and simulate versioning environment\n",
    "X, y = make_classification(n_samples=800, n_features=20, random_state=2024)\n",
    "clf = RandomForestClassifier(n_estimators=50, random_state=2024)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Saving model artifacts is necessary for model governance over time and helps us understand the “why” and the “what” behind its performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use X and y from cell 17 (already defined and used to train clf)\n",
    "Xv, _, yv, _ = train_test_split(X, y, test_size=0.8, random_state=2024)\n",
    "\n",
    "# Save the model artifact with versioned filename\n",
    "version = \"v1.0\"\n",
    "dump(clf, f\"rf_{version}.joblib\")\n",
    "\n",
    "# Save metadata about training environment\n",
    "meta = {\"version\": version, \"sklearn\": \"1.3.2\", \"numpy\": str(np.__version__)}\n",
    "with open(f\"rf_{version}_metadata.json\", \"w\") as f:\n",
    "    json.dump(meta, f)\n",
    "\n",
    "# Capture validation set performance snapshot\n",
    "yv_pred = clf.predict(Xv)\n",
    "val_acc = accuracy_score(yv, yv_pred)\n",
    "with open(f\"rf_{version}_val.txt\", \"w\") as f:\n",
    "    f.write(f\"{val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works...\n",
    "The scikit-learn roadmap recommends snapshotting not only the model but also the code, library versions, and a small validation set with its predicted outputs, to ensure the model can be validated after future upgrades or porting. Storing metadata with each artifact helps trace back issues, roll back to previous versions, and ensure reproducibility.\n",
    "\n",
    "# Setting Up Deployment Pipelines\n",
    "Deploying models reliably requires automation. In this recipe, we’ll show how to integrate model serialization, validation checks, and serving logic into a CI/CD pipeline using scikit-learn constructs, enabling consistent transitions from development to production.\n",
    "\n",
    "## Getting ready\n",
    "You’ll need tools to train a pipeline, export it, and programmatically validate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create data and pipeline\n",
    "X, y = make_classification(n_samples=500, n_features=10, random_state=2024)\n",
    "pipe = Pipeline([\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "Let’s add in a conditional at the end of our pipeline to test an arbitrary threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the entire pipeline\n",
    "dump(pipe, \"pipeline.joblib\")\n",
    "\n",
    "#Load the pipeline in deployment environment\n",
    "prod_pipe = load(\"pipeline.joblib\")\n",
    "\n",
    "# Simulate CI: validate accuracy on a test set\n",
    "Xt, yt = np.random.rand(100, 10), np.random.randint(0, 2, 100)\n",
    "acc = accuracy_score(yt, prod_pipe.predict(Xt))\n",
    "print(\"Validation accuracy:\", acc)\n",
    "\n",
    "# Conditionally deploy only if accuracy > threshold (e.g. > 0.8)\n",
    "if acc > 0.8:\n",
    "    print(\"Auto-deploy allowed\")\n",
    "else:\n",
    "    print(\"Halt deployment and review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Exercises in Model Deployment (Exercise Solutions)\n",
    "In this final section, we will engage in practical exercises that involve exporting and serving scikit-learn models, simulating live inference, and integrating monitoring and update strategies. These exercises are designed to consolidate our understanding of model deployment pipelines and demonstrate best practices for real-world ML operations. By the end of these exercises, we will have applied the full lifecycle of model deployment using scikit-learn.\n",
    "\n",
    "## Exercise 1: Saving and Reloading a Model Pipeline for Deployment\n",
    "In this exercise, we will serialize a trained model pipeline and reload it to simulate a production environment. This exercise ensures that preprocessing and inference logic are bundled together and reusable across environments.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from joblib import dump, load\n",
    "\n",
    "# Create and train pipeline\n",
    "X, y = make_classification(n_samples=500, n_features=10, random_state=2024)\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression())\n",
    "])\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# Save and reload pipeline\n",
    "dump(pipe, \"model_pipeline.joblib\")\n",
    "loaded_pipe = load(\"model_pipeline.joblib\")\n",
    "\n",
    "# Generate new predictions\n",
    "print(loaded_pipe.predict(X[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Monitoring Model Accuracy Over Time\n",
    "This exercise involves simulating streaming data and tracking a deployed model’s performance on incoming batches to detect model degradation.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the SGDClassifier hyperparameters to make the model less accurate\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=1.0, random_state=2024)  # Use SVM loss and strong regularization\n",
    "clf.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
    "batch_scores = []\n",
    "for Xb, yb in zip(stream_batches, stream_labels):\n",
    "    acc = np.mean(clf.predict(Xb) == yb)\n",
    "    batch_scores.append(acc)\n",
    "\n",
    "# Plot batch accuracy\n",
    "plt.plot(batch_scores, marker='o')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Batch Accuracy Over Time (High Regularization)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Automating Deployment Checks Using Validation Thresholds\n",
    "In this example, we’ll simulate a CI/CD deployment check that evaluates whether a serialized model meets performance criteria before promoting it to production.\n",
    "\n",
    "### Implementation Steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train and export model\n",
    "X, y = make_classification(n_samples=300, n_features=10, random_state=2024)\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "dump(model, \"model_ci.joblib\")\n",
    "\n",
    "# Load and validate with test data\n",
    "Xt, yt = make_classification(n_samples=100, n_features=10, random_state=2025)\n",
    "model_prod = load(\"model_ci.joblib\")\n",
    "pred = model_prod.predict(Xt)\n",
    "acc = accuracy_score(yt, pred)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# Conditionally approve deployment\n",
    "if acc > 0.8:\n",
    "    print(\"Deployment approved\")\n",
    "else:\n",
    "    print(\"Deployment rejected\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
